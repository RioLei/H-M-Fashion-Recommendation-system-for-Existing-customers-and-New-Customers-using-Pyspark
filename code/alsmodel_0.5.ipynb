{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2d3ec26",
   "metadata": {
    "papermill": {
     "duration": 0.013842,
     "end_time": "2023-02-20T11:54:14.782122",
     "exception": false,
     "start_time": "2023-02-20T11:54:14.768280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center> <h1 style=\"background-color:seagreen; color:white\" >Load Data</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00bb0fc1",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-02-20T11:54:14.813835Z",
     "iopub.status.busy": "2023-02-20T11:54:14.812540Z",
     "iopub.status.idle": "2023-02-20T11:55:15.290480Z",
     "shell.execute_reply": "2023-02-20T11:55:15.289773Z",
     "shell.execute_reply.started": "2023-02-20T11:07:33.999658Z"
    },
    "papermill": {
     "duration": 60.493832,
     "end_time": "2023-02-20T11:55:15.290675",
     "exception": false,
     "start_time": "2023-02-20T11:54:14.796843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Install PySpark\n",
    "!pip install pyspark > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2918342d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-02-20T11:55:15.322868Z",
     "iopub.status.busy": "2023-02-20T11:55:15.322162Z",
     "iopub.status.idle": "2023-02-20T11:55:15.384204Z",
     "shell.execute_reply": "2023-02-20T11:55:15.383550Z",
     "shell.execute_reply.started": "2023-02-20T11:08:10.843524Z"
    },
    "papermill": {
     "duration": 0.081245,
     "end_time": "2023-02-20T11:55:15.384364",
     "exception": false,
     "start_time": "2023-02-20T11:55:15.303119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Req libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96384cd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-20T11:55:15.415581Z",
     "iopub.status.busy": "2023-02-20T11:55:15.414783Z",
     "iopub.status.idle": "2023-02-20T11:55:21.355512Z",
     "shell.execute_reply": "2023-02-20T11:55:21.354822Z",
     "shell.execute_reply.started": "2023-02-20T11:08:10.909061Z"
    },
    "papermill": {
     "duration": 5.95863,
     "end_time": "2023-02-20T11:55:21.355680",
     "exception": false,
     "start_time": "2023-02-20T11:55:15.397050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 11:55:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Spark Session\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('fashion-recommendations').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7142cbf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-20T11:55:21.389360Z",
     "iopub.status.busy": "2023-02-20T11:55:21.388389Z",
     "iopub.status.idle": "2023-02-20T11:55:28.004421Z",
     "shell.execute_reply": "2023-02-20T11:55:28.003240Z",
     "shell.execute_reply.started": "2023-02-20T11:08:15.867350Z"
    },
    "papermill": {
     "duration": 6.63561,
     "end_time": "2023-02-20T11:55:28.004591",
     "exception": false,
     "start_time": "2023-02-20T11:55:21.368981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "transaction = spark.read.option('header','true').csv('../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71b8634",
   "metadata": {
    "papermill": {
     "duration": 0.012533,
     "end_time": "2023-02-20T11:55:28.031028",
     "exception": false,
     "start_time": "2023-02-20T11:55:28.018495",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center> <h1 style=\"background-color:seagreen; color:white\" >Feature Engineering</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb92de54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-20T11:55:28.066016Z",
     "iopub.status.busy": "2023-02-20T11:55:28.064992Z",
     "iopub.status.idle": "2023-02-20T11:56:34.073106Z",
     "shell.execute_reply": "2023-02-20T11:56:34.071924Z",
     "shell.execute_reply.started": "2023-02-20T11:08:21.250348Z"
    },
    "papermill": {
     "duration": 66.029108,
     "end_time": "2023-02-20T11:56:34.073319",
     "exception": false,
     "start_time": "2023-02-20T11:55:28.044211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===================================>                     (16 + 4) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 11:56:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:======================================================>  (25 + 1) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+\n",
      "|         customer_id|article_id|count|\n",
      "+--------------------+----------+-----+\n",
      "|0101b86c9ea4581da...|0822138002|    1|\n",
      "|05b6661e851f78bc0...|0783517002|    1|\n",
      "|16d13b0db3658a9fd...|0801327001|    1|\n",
      "|17f52fdadc1209858...|0762846008|    1|\n",
      "|1de87318baafea4da...|0788633001|    1|\n",
      "+--------------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# filter only 2020\n",
    "sales =  transaction.withColumn('t_dat', transaction['t_dat'].cast('string'))\n",
    "sales = sales.withColumn('date', from_unixtime(unix_timestamp('t_dat', 'yyyy-MM-dd')))\n",
    "sales = sales.withColumn('year', year(col('date')))\n",
    "sales = sales.withColumn('month', month(col('date')))\n",
    "\n",
    "# Let's filter the data to start with\n",
    "sales = sales[sales['year'] == 2020]\n",
    "sales = sales[sales['month'] == 1]\n",
    "\n",
    "transaction.unpersist()\n",
    "\n",
    "# Prepare the dataset\n",
    "sales = sales.groupby('customer_id', 'article_id').count()\n",
    "sales.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2941ee96",
   "metadata": {
    "papermill": {
     "duration": 0.019009,
     "end_time": "2023-02-20T11:56:34.112564",
     "exception": false,
     "start_time": "2023-02-20T11:56:34.093555",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center> <h1 style=\"background-color:seagreen; color:white\" >Alternative Least Sqaure</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a3b8343",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-20T11:56:34.161370Z",
     "iopub.status.busy": "2023-02-20T11:56:34.160046Z",
     "iopub.status.idle": "2023-02-20T11:56:34.463493Z",
     "shell.execute_reply": "2023-02-20T11:56:34.464043Z",
     "shell.execute_reply.started": "2023-02-20T11:09:06.848050Z"
    },
    "papermill": {
     "duration": 0.332633,
     "end_time": "2023-02-20T11:56:34.464234",
     "exception": false,
     "start_time": "2023-02-20T11:56:34.131601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b0458a",
   "metadata": {
    "papermill": {
     "duration": 0.018255,
     "end_time": "2023-02-20T11:56:34.501254",
     "exception": false,
     "start_time": "2023-02-20T11:56:34.482999",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center> <h1 style=\"background-color:DarkKhaki; color:white\" >Converting String to Index</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eab17311",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-20T11:56:34.544086Z",
     "iopub.status.busy": "2023-02-20T11:56:34.543107Z",
     "iopub.status.idle": "2023-02-20T11:59:33.715353Z",
     "shell.execute_reply": "2023-02-20T11:59:33.716374Z",
     "shell.execute_reply.started": "2023-02-20T11:09:07.109073Z"
    },
    "papermill": {
     "duration": 179.197061,
     "end_time": "2023-02-20T11:59:33.716720",
     "exception": false,
     "start_time": "2023-02-20T11:56:34.519659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:==================================>                     (16 + 4) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 11:59:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=====================================================>  (25 + 1) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 11:59:32 WARN DAGScheduler: Broadcasting large task binary with size 21.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+-----------------+----------------+\n",
      "|         customer_id|article_id|count|customer_id_index|article_id_index|\n",
      "+--------------------+----------+-----+-----------------+----------------+\n",
      "|0101b86c9ea4581da...|0822138002|    1|          14556.0|         10419.0|\n",
      "|05b6661e851f78bc0...|0783517002|    1|          39354.0|          1739.0|\n",
      "|16d13b0db3658a9fd...|0801327001|    1|           1107.0|          3228.0|\n",
      "|17f52fdadc1209858...|0762846008|    1|          70733.0|            61.0|\n",
      "|1de87318baafea4da...|0788633001|    1|           1274.0|          3222.0|\n",
      "+--------------------+----------+-----+-----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "indexer = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in list(set(sales.columns)-set(['count'])) ]\n",
    "pipeline = Pipeline(stages=indexer)\n",
    "transformed = pipeline.fit(sales).transform(sales)\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a05480",
   "metadata": {
    "papermill": {
     "duration": 0.033067,
     "end_time": "2023-02-20T11:59:33.795117",
     "exception": false,
     "start_time": "2023-02-20T11:59:33.762050",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center> <h1 style=\"background-color:DarkKhaki; color:white\" >Creating ALS model and fitting data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6747e534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-20T11:59:33.869843Z",
     "iopub.status.busy": "2023-02-20T11:59:33.868716Z",
     "iopub.status.idle": "2023-02-20T12:03:34.109273Z",
     "shell.execute_reply": "2023-02-20T12:03:34.107979Z",
     "shell.execute_reply.started": "2023-02-20T11:11:08.975821Z"
    },
    "papermill": {
     "duration": 240.281481,
     "end_time": "2023-02-20T12:03:34.109437",
     "exception": false,
     "start_time": "2023-02-20T11:59:33.827956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:==================================>                     (16 + 4) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:00:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:==================================>                     (16 + 4) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:01:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:=====================================================>  (25 + 1) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:01:28 WARN DAGScheduler: Broadcasting large task binary with size 21.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:01:30 WARN DAGScheduler: Broadcasting large task binary with size 21.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:01:33 WARN DAGScheduler: Broadcasting large task binary with size 21.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:01:36 WARN DAGScheduler: Broadcasting large task binary with size 21.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:01:41 WARN DAGScheduler: Broadcasting large task binary with size 21.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:01:43 WARN DAGScheduler: Broadcasting large task binary with size 21.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:01:47 WARN DAGScheduler: Broadcasting large task binary with size 21.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:01:49 WARN DAGScheduler: Broadcasting large task binary with size 21.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:01:58 WARN DAGScheduler: Broadcasting large task binary with size 21.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:02:10 WARN DAGScheduler: Broadcasting large task binary with size 21.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:02:18 WARN DAGScheduler: Broadcasting large task binary with size 21.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:02:29 WARN DAGScheduler: Broadcasting large task binary with size 21.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:02:36 WARN DAGScheduler: Broadcasting large task binary with size 21.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:02:49 WARN DAGScheduler: Broadcasting large task binary with size 21.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:02:56 WARN DAGScheduler: Broadcasting large task binary with size 21.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:03:07 WARN DAGScheduler: Broadcasting large task binary with size 21.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:03:15 WARN DAGScheduler: Broadcasting large task binary with size 21.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:03:26 WARN DAGScheduler: Broadcasting large task binary with size 21.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(training,test)=transformed.randomSplit([0.8, 0.2])\n",
    "\n",
    "als=ALS(maxIter=5,regParam=0.09,rank=25,userCol=\"customer_id_index\",itemCol=\"article_id_index\",ratingCol=\"count\",coldStartStrategy=\"drop\",nonnegative=True)\n",
    "model=als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff09fd8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-20T12:03:34.257969Z",
     "iopub.status.busy": "2023-02-20T12:03:34.256955Z",
     "iopub.status.idle": "2023-02-20T12:06:02.576334Z",
     "shell.execute_reply": "2023-02-20T12:06:02.577209Z",
     "shell.execute_reply.started": "2023-02-20T11:13:37.424657Z"
    },
    "papermill": {
     "duration": 148.395939,
     "end_time": "2023-02-20T12:06:02.577432",
     "exception": false,
     "start_time": "2023-02-20T12:03:34.181493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:>                (0 + 4) / 26][Stage 75:>                (0 + 0) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:03:35 WARN DAGScheduler: Broadcasting large task binary with size 21.4 MiB\n",
      "23/02/20 12:03:36 WARN DAGScheduler: Broadcasting large task binary with size 21.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:=>(16 + 4) / 26][Stage 75:>  (0 + 0) / 10][Stage 76:>  (0 + 0) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:04:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/02/20 12:04:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:=>(19 + 4) / 26][Stage 75:>  (0 + 0) / 10][Stage 76:>  (0 + 0) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:04:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:=====================================================>  (25 + 1) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:04:41 WARN DAGScheduler: Broadcasting large task binary with size 21.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 93:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:04:45 WARN DAGScheduler: Broadcasting large task binary with size 21.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE=0.514355310309057\n",
      "23/02/20 12:04:49 WARN DAGScheduler: Broadcasting large task binary with size 21.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 112:> (0 + 4) / 26][Stage 127:> (0 + 0) / 10][Stage 128:> (0 + 0) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:04:50 WARN DAGScheduler: Broadcasting large task binary with size 21.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 112:>(16 + 4) / 26][Stage 127:> (0 + 0) / 10][Stage 128:> (0 + 0) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:05:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/02/20 12:05:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 112:>(19 + 4) / 26][Stage 127:> (0 + 0) / 10][Stage 128:> (0 + 0) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:05:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 112:====================================================>  (25 + 1) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:05:57 WARN DAGScheduler: Broadcasting large task binary with size 21.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 145:=============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:06:01 WARN DAGScheduler: Broadcasting large task binary with size 21.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+-----------------+----------------+----------+\n",
      "|         customer_id|article_id|count|customer_id_index|article_id_index|prediction|\n",
      "+--------------------+----------+-----+-----------------+----------------+----------+\n",
      "|da624600418470043...|0708394003|    1|              1.0|          7518.0|0.85503507|\n",
      "|da624600418470043...|0744291012|    2|              1.0|          1995.0| 0.9797411|\n",
      "|da624600418470043...|0752814006|    1|              1.0|          1428.0| 0.9393404|\n",
      "|da624600418470043...|0318066004|    1|              1.0|           597.0|0.89577645|\n",
      "|da624600418470043...|0650279002|    1|              1.0|          3206.0|0.92523545|\n",
      "+--------------------+----------+-----+-----------------+----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"count\",predictionCol=\"prediction\")\n",
    "predictions=model.transform(test)\n",
    "rmse=evaluator.evaluate(predictions)\n",
    "print(\"RMSE=\"+str(rmse))\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f166a5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-20T12:06:02.807773Z",
     "iopub.status.busy": "2023-02-20T12:06:02.807013Z",
     "iopub.status.idle": "2023-02-20T12:07:14.277627Z",
     "shell.execute_reply": "2023-02-20T12:07:14.278208Z",
     "shell.execute_reply.started": "2023-02-20T11:18:14.720081Z"
    },
    "papermill": {
     "duration": 71.580021,
     "end_time": "2023-02-20T12:07:14.278403",
     "exception": false,
     "start_time": "2023-02-20T12:06:02.698382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 164:>               (0 + 4) / 26][Stage 179:>               (0 + 0) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:06:03 WARN DAGScheduler: Broadcasting large task binary with size 21.4 MiB\n",
      "23/02/20 12:06:04 WARN DAGScheduler: Broadcasting large task binary with size 21.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 164:>(16 + 4) / 26][Stage 179:> (0 + 0) / 10][Stage 180:> (0 + 0) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:06:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/02/20 12:06:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 164:>(18 + 4) / 26][Stage 179:> (0 + 0) / 10][Stage 180:> (0 + 0) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:06:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 164:====================================================>  (25 + 1) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:07:10 WARN DAGScheduler: Broadcasting large task binary with size 21.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 197:=============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:07:13 WARN DAGScheduler: Broadcasting large task binary with size 21.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 215:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|         customer_id|prediction|\n",
      "+--------------------+----------+\n",
      "|da624600418470043...|0.85503507|\n",
      "|da624600418470043...| 0.9797411|\n",
      "|da624600418470043...| 0.9393404|\n",
      "|da624600418470043...|0.89577645|\n",
      "|da624600418470043...|0.92523545|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = predictions.select('customer_id', 'prediction')\n",
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8388bd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-20T12:07:14.508247Z",
     "iopub.status.busy": "2023-02-20T12:07:14.507572Z",
     "iopub.status.idle": "2023-02-20T12:08:30.220617Z",
     "shell.execute_reply": "2023-02-20T12:08:30.219798Z",
     "shell.execute_reply.started": "2023-02-20T11:19:40.157704Z"
    },
    "papermill": {
     "duration": 75.830021,
     "end_time": "2023-02-20T12:08:30.220836",
     "exception": false,
     "start_time": "2023-02-20T12:07:14.390815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 216:>               (0 + 2) / 26][Stage 231:>               (0 + 0) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:07:15 WARN DAGScheduler: Broadcasting large task binary with size 21.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 216:>               (0 + 4) / 26][Stage 231:>               (0 + 0) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:07:16 WARN DAGScheduler: Broadcasting large task binary with size 21.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 216:>(16 + 4) / 26][Stage 231:> (0 + 0) / 10][Stage 232:> (0 + 0) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:07:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/02/20 12:07:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 216:>(17 + 4) / 26][Stage 231:> (0 + 0) / 10][Stage 232:> (0 + 0) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:07:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 216:====================================================>  (25 + 1) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:08:22 WARN DAGScheduler: Broadcasting large task binary with size 21.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 249:=============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 12:08:26 WARN DAGScheduler: Broadcasting large task binary with size 21.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "my_pred = result.toPandas()\n",
    "my_pred.to_csv('/kaggle/working/submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e500b8",
   "metadata": {
    "papermill": {
     "duration": 0.125301,
     "end_time": "2023-02-20T12:08:30.476222",
     "exception": false,
     "start_time": "2023-02-20T12:08:30.350921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 866.57528,
   "end_time": "2023-02-20T12:08:31.329414",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-02-20T11:54:04.754134",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
