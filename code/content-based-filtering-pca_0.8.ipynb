{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Building recommender system using content based filtering approach**\n\nThe following code:\n\n* Builds weighted one-hot endcoded item embeddings in the feature space\n* Projecting customers into the embeddings space\n* Performing dimensionality reduction usng PCA and picking the first 150 principal component\n* Finds N similar items using ApproximateNearestKneighbor from spark MLLib\n\nCold start approach: recommend most frequent items.\n\nInput data limited to 10000 transactions due to memory constraints","metadata":{}},{"cell_type":"code","source":"!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2023-02-27T05:57:33.063546Z","iopub.execute_input":"2023-02-27T05:57:33.063852Z","iopub.status.idle":"2023-02-27T05:58:18.721449Z","shell.execute_reply.started":"2023-02-27T05:57:33.063822Z","shell.execute_reply":"2023-02-27T05:58:18.720343Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n     |████████████████████████████████| 281.4 MB 8.7 kB/s             \n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting py4j==0.10.9.5\n  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n     |████████████████████████████████| 199 kB 44.3 MB/s            \n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824025 sha256=4248697715cb37285103b8737717c3f2feb2766b567258c994dade3297f879a5\n  Stored in directory: /root/.cache/pip/wheels/07/fb/67/b9f2c0242d156eaa136b45ae4fd99d3e7c0ecc2acfd26f47b9\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\n  Attempting uninstall: py4j\n    Found existing installation: py4j 0.10.9.4\n    Uninstalling py4j-0.10.9.4:\n      Successfully uninstalled py4j-0.10.9.4\nSuccessfully installed py4j-0.10.9.5 pyspark-3.3.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import *\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom skimage import io\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.sql import Row\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import col, lit, lower\nfrom pyspark.ml.feature import BucketedRandomProjectionLSH","metadata":{"execution":{"iopub.status.busy":"2023-02-27T05:58:18.723729Z","iopub.execute_input":"2023-02-27T05:58:18.724011Z","iopub.status.idle":"2023-02-27T05:58:19.411124Z","shell.execute_reply.started":"2023-02-27T05:58:18.723975Z","shell.execute_reply":"2023-02-27T05:58:19.410401Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql.functions import col\nfrom pyspark.sql import SparkSession\n\n# spark = SparkSession.builder.appName('Recommendations').getOrCreate()\n\nspark = (SparkSession.builder.appName(\"HM-Recommendations\").config(\"spark.executor.memory\", \"14g\").getOrCreate())\n\nfeatures = ['article_id', 'prod_name', 'product_type_name','product_group_name', 'graphical_appearance_name', 'colour_group_name',\n            'perceived_colour_value_name','perceived_colour_master_name','department_name', 'index_name','index_group_name', 'section_name',\n            'garment_group_name', 'detail_desc']\n\nfeature_subset = ['product_group_name', 'graphical_appearance_name', 'colour_group_name', 'perceived_colour_value_name',\n                  'perceived_colour_master_name', 'department_name', 'index_name', 'index_group_name', 'section_name', 'garment_group_name']\n    \n\nrcmnds = spark.read.options(header=True).csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv').select('customer_id')\nrcmnds.show(5)\n\ntransactions = spark.read.options(header=True).csv(\"../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv\")\\\n                    .drop('sales_channel_id').drop('price').limit(10000)\ntransactions.show(5)\n# df = spark.read.csv('../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv', header=True, inferSchema=True)\n\nitems = spark.read.options(header=True).csv(\"../input/h-and-m-personalized-fashion-recommendations/articles.csv\").select(features)\nitems.show(5)\n# articles = spark.read.csv('../input/h-and-m-personalized-fashion-recommendations/articles.csv', header=True, inferSchema=True)\n\n# df = users.join(articles, on='article_id')\n\n# df = df.select('t_dat', 'customer_id', 'article_id', 'prod_name', 'product_type_name', 'product_group_name',\n# 'graphical_appearance_name', 'colour_group_name', 'perceived_colour_value_name', 'perceived_colour_master_name',\n# 'department_name', 'index_name', 'index_group_name', 'section_name', 'garment_group_name', 'detail_desc')\n\n# df.show(5)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T05:58:19.412557Z","iopub.execute_input":"2023-02-27T05:58:19.412810Z","iopub.status.idle":"2023-02-27T05:58:33.831291Z","shell.execute_reply.started":"2023-02-27T05:58:19.412775Z","shell.execute_reply":"2023-02-27T05:58:33.830456Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","output_type":"stream"},{"name":"stdout","text":"23/02/27 05:58:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+--------------------+\n|         customer_id|\n+--------------------+\n|00000dbacae5abe5e...|\n|0000423b00ade9141...|\n|000058a12d5b43e67...|\n|00005ca1c9ed5f514...|\n|00006413d8573cd20...|\n+--------------------+\nonly showing top 5 rows\n\n+----------+--------------------+----------+\n|     t_dat|         customer_id|article_id|\n+----------+--------------------+----------+\n|2018-09-20|000058a12d5b43e67...|0663713001|\n|2018-09-20|000058a12d5b43e67...|0541518023|\n|2018-09-20|00007d2de826758b6...|0505221004|\n|2018-09-20|00007d2de826758b6...|0685687003|\n|2018-09-20|00007d2de826758b6...|0685687004|\n+----------+--------------------+----------+\nonly showing top 5 rows\n\n+----------+-----------------+-----------------+------------------+-------------------------+-----------------+---------------------------+----------------------------+---------------+----------------+----------------+--------------------+------------------+--------------------+\n|article_id|        prod_name|product_type_name|product_group_name|graphical_appearance_name|colour_group_name|perceived_colour_value_name|perceived_colour_master_name|department_name|      index_name|index_group_name|        section_name|garment_group_name|         detail_desc|\n+----------+-----------------+-----------------+------------------+-------------------------+-----------------+---------------------------+----------------------------+---------------+----------------+----------------+--------------------+------------------+--------------------+\n|0108775015|        Strap top|         Vest top|Garment Upper body|                    Solid|            Black|                       Dark|                       Black|   Jersey Basic|      Ladieswear|      Ladieswear|Womens Everyday B...|      Jersey Basic|Jersey top with n...|\n|0108775044|        Strap top|         Vest top|Garment Upper body|                    Solid|            White|                      Light|                       White|   Jersey Basic|      Ladieswear|      Ladieswear|Womens Everyday B...|      Jersey Basic|Jersey top with n...|\n|0108775051|    Strap top (1)|         Vest top|Garment Upper body|                   Stripe|        Off White|                Dusty Light|                       White|   Jersey Basic|      Ladieswear|      Ladieswear|Womens Everyday B...|      Jersey Basic|Jersey top with n...|\n|0110065001|OP T-shirt (Idro)|              Bra|         Underwear|                    Solid|            Black|                       Dark|                       Black| Clean Lingerie|Lingeries/Tights|      Ladieswear|     Womens Lingerie| Under-, Nightwear|Microfibre T-shir...|\n|0110065002|OP T-shirt (Idro)|              Bra|         Underwear|                    Solid|            White|                      Light|                       White| Clean Lingerie|Lingeries/Tights|      Ladieswear|     Womens Lingerie| Under-, Nightwear|Microfibre T-shir...|\n+----------+-----------------+-----------------+------------------+-------------------------+-----------------+---------------------------+----------------------------+---------------+----------------+----------------+--------------------+------------------+--------------------+\nonly showing top 5 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# import numpy as np \n# import pandas as pd \n# import matplotlib.pyplot as plt\n# import plotly.graph_objects as go\n# from skimage import io\n\n# df = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv', chunksize=100000)\n# articles = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/articles.csv')\n# users = next(df)\n# df = users.merge(articles, on='article_id')\n# df = df[['t_dat', 'customer_id', 'article_id', 'prod_name', 'product_type_name',\n#        'product_group_name', \n#        'graphical_appearance_name', 'colour_group_name',\n#        'perceived_colour_value_name',\n#        'perceived_colour_master_name',\n#        'department_name', 'index_name',\n#        'index_group_name', 'section_name',\n#        'garment_group_name', 'detail_desc']]\n\n# feature_subset = ['product_group_name', \n#        'graphical_appearance_name', 'colour_group_name',\n#        'perceived_colour_value_name',\n#        'perceived_colour_master_name',\n#        'department_name', 'index_name',\n#        'index_group_name', 'section_name',\n#        'garment_group_name']\n\n# df.head()\n\n# #Choose features to build feature space\n# features = feature_subset\n# df1 = df[['customer_id', 'article_id'] + features]\n# dummies_df = pd.get_dummies(df1, columns=features)\n# dummies_df\n\n# minimum_items = 2\n# groupby_customer = dummies_df.groupby('customer_id')\n\n# l = []\n# cutomer_ids = []\n# article_ids = []\n# for key in groupby_customer.groups.keys():\n#     temp = groupby_customer.get_group(key)\n#     if temp.article_id.nunique() >= minimum_items:\n#         l.append(temp.drop('article_id', axis=1).sum(numeric_only=True).values)\n#         cutomer_ids.append(key)\n#         article_ids.extend(temp.article_id.values.tolist())\n\n# user_feature = pd.DataFrame(l, columns = dummies_df.columns[2:])\n# normalized_user_feature = user_feature.div(user_feature.sum(axis=1), axis=0)\n# normalized_user_feature.insert(0, 'customer_id', cutomer_ids)\n# normalized_user_feature = normalized_user_feature.set_index('customer_id')\n# normalized_user_feature\n\n# item_feature = dummies_df.drop_duplicates(subset='article_id')\n# item_feature = item_feature[item_feature.article_id.isin(article_ids)].drop('customer_id', axis=1)\n# item_feature = item_feature.set_index('article_id')\n# item_feature\n\n# scores = normalized_user_feature.dot(item_feature.T)\n# scores\n\n# def get_rcmnd(customer_id, scores):\n#     cutomer_scores = scores.loc[customer_id]\n#     customer_prev_items = groupby_customer.get_group(customer_id)['article_id']\n#     prev_dropped = cutomer_scores.drop(customer_prev_items.values)\n#     ordered = prev_dropped.sort_values(ascending=False)   \n#     return ordered, customer_prev_items\n\n# def plot_prev(prev_items):\n#     fig = plt.figure(figsize=(20, 10))\n#     for item, i in zip(prev_items, range(1, len(prev_items)+1)):\n#         item = '0' + str(item)\n#         sub = item[:3]\n#         image = path + \"/\"+ sub + \"/\"+ item +\".jpg\"\n#         image = plt.imread(image)\n#         fig.add_subplot(1, 6, i)\n#         plt.imshow(image)\n\n# def plot_rcmnd(rcmnds):\n#     fig = plt.figure(figsize=(20, 10))\n#     for item, i in zip(rcmnds, range(1, k+1)):\n#         item = '0' + str(item)\n#         sub = item[:3]\n#         image = path + \"/\"+ sub + \"/\"+ item +\".jpg\"\n#         image = plt.imread(image)\n#         fig.add_subplot(1, 6, i)\n#         plt.imshow(image)\n\n# from sklearn.decomposition import PCA\n# pca = PCA(n_components=100)\n# pca.fit(normalized_user_feature)\n# pca.explained_variance_ratio_.sum()\n\n# user_feature_pca = pd.DataFrame(pca.transform(normalized_user_feature), columns=['component_{}'.format(i) for i in range(1, 101)]).set_index(normalized_user_feature.index)\n# item_feature_pca = pd.DataFrame(pca.transform(item_feature), columns=['component_{}'.format(i) for i in range(1, 101)]).set_index(item_feature.index)\n\n# scores_pca = user_feature_pca.dot(item_feature_pca.T)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-27T05:58:33.835652Z","iopub.execute_input":"2023-02-27T05:58:33.835909Z","iopub.status.idle":"2023-02-27T05:58:33.854638Z","shell.execute_reply.started":"2023-02-27T05:58:33.835875Z","shell.execute_reply":"2023-02-27T05:58:33.853905Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# features = feature_subset\n# df1 = df.select(['customer_id', 'article_id'] + features)\n# df1.show\n#Choose features to build feature space\n# features = feature_subset\n# df1 = df.select('customer_id', 'article_id', *features)\n# dummies_df = df1.select('customer_id', 'article_id', *[udf(lambda x: int(bool(x)), IntegerType())(col(c)).alias(c) for c in features])\n# dummies_df = pd.get_dummies(dummies_df.toPandas(), columns=features)\n# dummies_df\n\n# minimum_items = 2\n# groupby_customer = dummies_df.groupby('customer_id')\n\n# l = []\n# cutomer_ids = []\n# article_ids = []\n# for key in groupby_customer.groups.keys():\n#     temp = groupby_customer.get_group(key)\n#     if temp.article_id.nunique() >= minimum_items:\n#         l.append(temp.drop('article_id', axis=1).sum(numeric_only=True).values)\n#         cutomer_ids.append(key)\n#         article_ids.extend(temp.article_id.values.tolist())\n        \n# user_feature = pd.DataFrame(l, columns = dummies_df.columns[2:])\n# normalized_user_feature = user_feature.div(user_feature.sum(axis=1), axis=0)\n# normalized_user_feature.insert(0, 'customer_id', cutomer_ids)\n# normalized_user_feature = normalized_user_feature.set_index('customer_id')\n# print(normalized_user_feature)\n\n# item_feature = dummies_df.drop_duplicates(subset='article_id')\n# item_feature = item_feature[item_feature.article_id.isin(article_ids)].drop('customer_id', axis=1)\n# item_feature = item_feature.set_index('article_id')\n# print(item_feature)\n\n# scores = normalized_user_feature.dot(item_feature.T)\n# print(scores)\n\n# def get_rcmnd(customer_id, scores):\n#     cutomer_scores = scores.loc[customer_id]\n#     customer_prev_items = groupby_customer.get_group(customer_id)['article_id']\n#     prev_dropped = cutomer_scores.drop(customer_prev_items.values)\n#     ordered = prev_dropped.sort_values(ascending=False)\n#     return ordered, customer_prev_items\n\n# def plot_prev(prev_items):\n#     fig = plt.figure(figsize=(20, 10))\n#     for item, i in zip(prev_items, range(1, len(prev_items)+1)):\n#         item = '0' + str(item)\n#         sub = item[:3]\n#         image = path + \"/\"+ sub + \"/\"+ item +\".jpg\"\n#         image = plt.imread(image)\n#         fig.add_subplot(1, 6, i)\n#         plt.imshow(image)\n    \n# def plot_rcmnd(rcmnds):\n#     fig = plt.figure(figsize=(20, 10))\n#     for item, i in zip(rcmnds, range(1, k+1)):\n#         item = '0' + str(item)\n#         sub = item[:3]\n#         image = path + \"/\"+ sub + \"/\"+ item +\".jpg\"\n#         image = plt.imread(image)\n#         fig.add_subplot(1, 6, i)\n#         plt.imshow(image)\n        \n        \n# from pyspark.ml.feature import PCA\n\n# pca = PCA(k=100, inputCol='normalized_user_feature', outputCol='user_feature_pca')\n# pca_model = pca.fit(user_feature)\n# user_feature_pca = pca_model.transform(user_feature).select('user_feature_pca')","metadata":{"execution":{"iopub.status.busy":"2023-02-27T05:58:33.859809Z","iopub.execute_input":"2023-02-27T05:58:33.862008Z","iopub.status.idle":"2023-02-27T05:58:34.802538Z","shell.execute_reply.started":"2023-02-27T05:58:33.861971Z","shell.execute_reply":"2023-02-27T05:58:34.801609Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def to_lower(items):\n    for c in feature_subset:\n        items = items.withColumn(c, lower(col(c)))\n    \n    return items\n\ndef ohe(items):\n    keys = ['article_id']\n    def join_all(dfs,keys):\n        if len(dfs) > 1:\n            return dfs[0].join(join_all(dfs[1:],keys), on = keys, how = 'inner')\n        else:\n            return dfs[0]\n\n    dfs = []\n    combined = []\n    for pivot_col in feature_subset:\n        pivotDF = items.groupBy(keys).pivot(pivot_col).count()\n        new_names = pivotDF.columns[:len(keys)] +  [\"e_{0}_{1}\".format(pivot_col, i) for i, c in enumerate(pivotDF.columns[len(keys):])]        \n        newdf = pivotDF.toDF(*new_names).fillna(0)    \n        combined.append(newdf)\n\n    item_feature = join_all(combined,keys)\n    \n    return item_feature\n\nitems = to_lower(items)\nitem_feature = ohe(items)\n# print(item_feature)\n\ntransactions = transactions.join(item_feature, on='article_id', how='left').sort('t_dat').drop(*features[1:])\n\ndummy_features = transactions.columns[3:]\n# print(dummy_features)\n\nuser_feature = transactions.groupBy('customer_id').sum(*dummy_features)\n# print(user_feature)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-02-27T05:58:34.803813Z","iopub.execute_input":"2023-02-27T05:58:34.804589Z","iopub.status.idle":"2023-02-27T05:58:49.971232Z","shell.execute_reply.started":"2023-02-27T05:58:34.804548Z","shell.execute_reply":"2023-02-27T05:58:49.970412Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\nfrom pyspark.ml import Pipeline\n\ndef scale(df, col):\n    \n    assembler = VectorAssembler(inputCols=df.columns[1:], outputCol=\"sparse_features\")\n    feature_vectors = assembler.transform(df).select(*(col, \"sparse_features\"))\n\n    scaler = StandardScaler(inputCol=\"sparse_features\", outputCol=\"scaled_features\")\n    scalerModel = scaler.fit(feature_vectors)\n    \n    scaled_feature_vectors = scalerModel.transform(feature_vectors).select(*(col, \"scaled_features\"))\n    \n    return scaled_feature_vectors\n\n\ndef get_pca(df, col):\n    \n    pca = PCA(k=100, inputCol=\"scaled_features\", outputCol=\"pca\")\n    pcaModel = pca.fit(df)\n    \n    return pcaModel\n\nscaled_user_feature = scale(user_feature, 'customer_id')\nscaled_item_feature = scale(item_feature, 'article_id')\n\npca_model = get_pca(scaled_user_feature, 'customer_id')\nprint(pca_model)\n\nuser_feature_pca = pca_model.transform(scaled_user_feature)\nprint(user_feature_pca)\n\nitem_feature_pca = pca_model.transform(scaled_item_feature)\nprint(item_feature_pca)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T05:58:49.972267Z","iopub.execute_input":"2023-02-27T05:58:49.972497Z","iopub.status.idle":"2023-02-27T06:02:29.767899Z","shell.execute_reply.started":"2023-02-27T05:58:49.972457Z","shell.execute_reply":"2023-02-27T06:02:29.767120Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"23/02/27 05:58:52 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n","output_type":"stream"},{"name":"stderr","text":"[Stage 118:>                                                        (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/27 05:59:16 WARN DAGScheduler: Broadcasting large task binary with size 1090.9 KiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"23/02/27 05:59:58 WARN DAGScheduler: Broadcasting large task binary with size 1091.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 164:============================>                            (1 + 1) / 2]\r","output_type":"stream"},{"name":"stdout","text":"23/02/27 06:00:25 WARN DAGScheduler: Broadcasting large task binary with size 1241.9 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 188:>                                                        (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/27 06:00:30 WARN DAGScheduler: Broadcasting large task binary with size 1790.3 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 213:>                                                        (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/27 06:00:33 WARN DAGScheduler: Broadcasting large task binary with size 1904.1 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 269:>                                                        (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/27 06:00:50 WARN DAGScheduler: Broadcasting large task binary with size 1160.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 290:============================>                            (1 + 1) / 2]\r","output_type":"stream"},{"name":"stdout","text":"23/02/27 06:01:21 WARN DAGScheduler: Broadcasting large task binary with size 1196.2 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 345:>                                                        (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/27 06:01:37 WARN DAGScheduler: Broadcasting large task binary with size 1087.8 KiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"23/02/27 06:01:58 WARN DAGScheduler: Broadcasting large task binary with size 1088.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 391:============================>                            (1 + 1) / 2]\r","output_type":"stream"},{"name":"stdout","text":"23/02/27 06:02:16 WARN DAGScheduler: Broadcasting large task binary with size 1238.8 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 415:>                                                        (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/27 06:02:19 WARN DAGScheduler: Broadcasting large task binary with size 1667.8 KiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"23/02/27 06:02:20 WARN DAGScheduler: Broadcasting large task binary with size 1667.8 KiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"23/02/27 06:02:21 WARN DAGScheduler: Broadcasting large task binary with size 1669.8 KiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"23/02/27 06:02:24 WARN DAGScheduler: Broadcasting large task binary with size 1668.2 KiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"23/02/27 06:02:25 WARN DAGScheduler: Broadcasting large task binary with size 1668.9 KiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"23/02/27 06:02:27 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n23/02/27 06:02:27 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\nPCAModel: uid=PCA_dbfd5acf29c6, k=100\nDataFrame[customer_id: string, scaled_features: vector, pca: vector]\nDataFrame[article_id: string, scaled_features: vector, pca: vector]\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.ml.feature import BucketedRandomProjectionLSH\nfrom pyspark.sql.functions import col, udf\nimport pyspark.sql.functions as F\n\nflagged = rcmnds.join(user_feature_pca.withColumn('flag', F.lit(True)), 'customer_id', 'left').fillna(False)\nprint(flagged)\n\ncold_start = flagged.where('!flag').drop('flag')\nprint(cold_start)\n\nwith_history = flagged.where('flag').drop('flag')\nprint(with_history)\n\nrows = with_history.collect()\nprint(rows[0])\n\ndef get_rcmnds(customer, k=12):\n    brp = BucketedRandomProjectionLSH(inputCol=\"pca\", outputCol=\"hashes\", seed=12345, bucketLength=1.0)\n    model = brp.fit(user_feature_pca)\n    temp = model.approxNearestNeighbors(item_feature_pca, customer.pca, k).select('article_id').collect()\n    return temp\n\ncustomers = []\nitems = []\nfor row in rows:\n    temp = get_rcmnds(row)\n    customers.append(row[0])\n    items.append(' '.join([i[0] for i in temp]))\n\n# from pyspark.sql.functions import udf\n# from pyspark.sql.types import StringType\n\n# # Định nghĩa hàm user-defined để lấy kết quả tư vấn cho từng khách hàng\n# @udf(returnType=StringType())\n# def get_rcmnds_udf(customer):\n#     brp = BucketedRandomProjectionLSH(inputCol=\"pca\", outputCol=\"hashes\", seed=12345, bucketLength=1.0)\n#     model = brp.fit(user_feature_pca)\n#     temp = model.approxNearestNeighbors(item_feature_pca, customer.pca, k).select('article_id').collect()\n#     return ' '.join([i[0] for i in temp])\n\n# # Tách dữ liệu đầu vào thành các phần nhỏ để xử lý song song\n# num_partitions = 4\n# partitioned_data = rows.repartition(num_partitions)\n\n# # Sử dụng hàm user-defined và caching để lưu trữ kết quả tính toán\n# partitioned_data = partitioned_data.withColumn('items', get_rcmnds_udf(partitioned_data[0])).cache()\n\n# # Thu thập kết quả tính toán và nối chúng lại\n# customers = [row[0] for row in partitioned_data.collect()]\n# items = [' '.join(row[1].split()) for row in partitioned_data.collect()]\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-02-27T06:07:32.651002Z","iopub.execute_input":"2023-02-27T06:07:32.651292Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"DataFrame[customer_id: string, scaled_features: vector, pca: vector, flag: boolean]\nDataFrame[customer_id: string, scaled_features: vector, pca: vector]\nDataFrame[customer_id: string, scaled_features: vector, pca: vector]\n","output_type":"stream"},{"name":"stderr","text":"[Stage 704:>                                                        (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/27 06:07:54 WARN DAGScheduler: Broadcasting large task binary with size 1090.9 KiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"23/02/27 06:08:21 WARN DAGScheduler: Broadcasting large task binary with size 1091.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 750:============================>                            (1 + 1) / 2]\r","output_type":"stream"},{"name":"stdout","text":"23/02/27 06:08:42 WARN DAGScheduler: Broadcasting large task binary with size 1241.9 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 774:>                                                        (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/27 06:08:46 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Row(customer_id='000058a12d5b43e67d225668fa1f8d618c13dc232df0cad8ffe7ad4a1091e318', scaled_features=SparseVector(467, {16: 2.3241, 44: 0.9758, 50: 0.6153, 73: 2.609, 100: 0.4532, 101: 1.0403, 108: 0.6199, 119: 1.8555, 169: 3.4127, 192: 1.578, 382: 1.9109, 387: 0.8273, 436: 2.5315, 464: 2.1834}), pca=DenseVector([-1.5512, 0.7372, 1.9728, -2.3521, -2.3177, 0.2438, 1.5743, -1.258, -2.4537, -0.3919, 1.0904, -0.2851, -0.2111, 0.0758, 0.4033, 0.6975, 0.0137, 0.1488, 0.0697, -0.3862, -0.081, -0.149, -0.1779, -0.0541, -0.0647, 0.1924, 0.0005, 0.8418, 0.5, 0.0659, -0.0915, 0.1752, 0.2201, -0.2583, -0.3694, -0.4289, 0.2696, 0.2568, -0.212, -0.1437, 0.0077, 0.2443, 0.0297, 0.0317, 0.1871, -0.3317, -0.3604, -0.3497, -0.5827, 0.0597, 0.1464, 0.0459, -0.4952, 0.1354, 0.2018, 0.1793, 0.0304, -0.2281, -0.0038, 0.4217, -0.2414, 0.0497, -0.3532, -0.6692, -0.1722, -0.0783, 0.326, -0.689, -0.1874, -0.3543, 0.1921, -0.3941, 0.1569, 0.114, -0.5421, 0.0324, -0.6049, -0.9496, 0.1455, -0.5577, 0.406, -0.0216, 0.5128, -0.305, -0.0712, -0.3063, -0.4399, 0.1177, -0.1522, -0.0867, -0.3426, 0.0825, -0.257, 0.7308, -0.1852, -0.2055, -0.2351, -0.092, 0.173, 0.4255]))\n","output_type":"stream"},{"name":"stderr","text":"[Stage 830:>                                                        (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/27 06:09:06 WARN DAGScheduler: Broadcasting large task binary with size 1489.0 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 881:>                                                        (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/27 06:10:03 WARN DAGScheduler: Broadcasting large task binary with size 1489.0 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 902:>                                                        (0 + 2) / 2]\r","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nwith_history_df = pd.DataFrame({'customer_id':customers, 'items':items})\n\nmost_freq = transactions.groupBy('article_id').count().sort(col('count').desc()).limit(12).collect()\nprint(most_freq)\n\ndefault = [i[0] for i in most_freq]\nprint(default)\n\n\n\ncold_start = cold_start.withColumn('items', lit(' '.join(default))).select(*('customer_id', 'items')).toPandas()\ncold_start.head()\n\nall_rcmnds = cold_start.append(with_history_df)\nprint(all_rcmnds)\n\nevaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"count\",predictionCol=\"prediction\")\nrmse=evaluator.evaluate(all_rcmnds)\nprint(rmse)\n# all_rcmnds.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T06:03:49.115486Z","iopub.status.idle":"2023-02-27T06:03:49.117967Z","shell.execute_reply.started":"2023-02-27T06:03:49.117711Z","shell.execute_reply":"2023-02-27T06:03:49.117740Z"},"trusted":true},"execution_count":null,"outputs":[]}]}