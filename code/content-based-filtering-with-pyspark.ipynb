{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Building recommender system using content based filtering approach**\n\nThe following code:\n\n* Builds weighted one-hot endcoded item embeddings in the feature space\n* Projecting customers into the embeddings space\n* Performing dimensionality reduction usng PCA and picking the first 150 principal component\n* Finds N similar items using ApproximateNearestKneighbor from spark MLLib\n\nCold start approach: recommend most frequent items.\n\nInput data limited to 10000 transactions due to memory constraints\n\n","metadata":{}},{"cell_type":"code","source":"!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:27:15.791289Z","iopub.execute_input":"2023-02-22T05:27:15.791569Z","iopub.status.idle":"2023-02-22T05:27:24.891531Z","shell.execute_reply.started":"2023-02-22T05:27:15.791535Z","shell.execute_reply":"2023-02-22T05:27:24.890616Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyspark in /opt/conda/lib/python3.7/site-packages (3.3.2)\nRequirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.7/site-packages (from pyspark) (0.10.9.5)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.sql import Row\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import col, lit, lower\nfrom pyspark.ml.feature import BucketedRandomProjectionLSH\n\nfeatures = ['article_id', 'prod_name', 'product_type_name',\n       'product_group_name', \n       'graphical_appearance_name', 'colour_group_name',\n       'perceived_colour_value_name',\n       'perceived_colour_master_name',\n       'department_name', 'index_name',\n       'index_group_name', 'section_name',\n       'garment_group_name', 'detail_desc']\n\npivot_cols = ['product_group_name', \n       'graphical_appearance_name', 'colour_group_name',\n       'perceived_colour_value_name',\n       'perceived_colour_master_name',\n       'department_name', 'index_name',\n       'index_group_name', 'section_name',\n       'garment_group_name']\n\nspark = SparkSession.builder.appName('Recommendations').getOrCreate()\n\ntransactions = spark.read.options(header=True).csv(\n    \"../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv\").drop(\n    'sales_channel_id').drop('price').limit(10000)\n    \n\nitems = spark.read.options(header=True).csv(\n    \"../input/h-and-m-personalized-fashion-recommendations/articles.csv\").select(features)\n\nrcmnds = spark.read.options(header=True).csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv'\n                       ).select('customer_id')","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:27:24.893301Z","iopub.execute_input":"2023-02-22T05:27:24.893558Z","iopub.status.idle":"2023-02-22T05:27:38.843599Z","shell.execute_reply.started":"2023-02-22T05:27:24.893525Z","shell.execute_reply":"2023-02-22T05:27:38.842527Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:27:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}]},{"cell_type":"code","source":"transactions","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:27:38.847084Z","iopub.execute_input":"2023-02-22T05:27:38.847772Z","iopub.status.idle":"2023-02-22T05:27:38.914718Z","shell.execute_reply.started":"2023-02-22T05:27:38.847702Z","shell.execute_reply":"2023-02-22T05:27:38.913893Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DataFrame[t_dat: string, customer_id: string, article_id: string]"},"metadata":{}}]},{"cell_type":"code","source":"items","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:27:38.919947Z","iopub.execute_input":"2023-02-22T05:27:38.922010Z","iopub.status.idle":"2023-02-22T05:27:38.932403Z","shell.execute_reply.started":"2023-02-22T05:27:38.921949Z","shell.execute_reply":"2023-02-22T05:27:38.931231Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DataFrame[article_id: string, prod_name: string, product_type_name: string, product_group_name: string, graphical_appearance_name: string, colour_group_name: string, perceived_colour_value_name: string, perceived_colour_master_name: string, department_name: string, index_name: string, index_group_name: string, section_name: string, garment_group_name: string, detail_desc: string]"},"metadata":{}}]},{"cell_type":"code","source":"rcmnds","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:27:38.934803Z","iopub.execute_input":"2023-02-22T05:27:38.935932Z","iopub.status.idle":"2023-02-22T05:27:38.950445Z","shell.execute_reply.started":"2023-02-22T05:27:38.935884Z","shell.execute_reply":"2023-02-22T05:27:38.949541Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DataFrame[customer_id: string]"},"metadata":{}}]},{"cell_type":"code","source":"def to_lower(items):\n    for c in pivot_cols:\n        items = items.withColumn(c, lower(col(c)))\n    \n    return items","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:27:38.953398Z","iopub.execute_input":"2023-02-22T05:27:38.956703Z","iopub.status.idle":"2023-02-22T05:27:38.962515Z","shell.execute_reply.started":"2023-02-22T05:27:38.956649Z","shell.execute_reply":"2023-02-22T05:27:38.961529Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def ohe(items):\n    keys = ['article_id']\n    def join_all(dfs,keys):\n        if len(dfs) > 1:\n            return dfs[0].join(join_all(dfs[1:],keys), on = keys, how = 'inner')\n        else:\n            return dfs[0]\n\n    dfs = []\n    combined = []\n    for pivot_col in pivot_cols:\n        pivotDF = items.groupBy(keys).pivot(pivot_col).count()\n        new_names = pivotDF.columns[:len(keys)] +  [\"e_{0}_{1}\".format(pivot_col, i) for i, c in enumerate(pivotDF.columns[len(keys):])]        \n        newdf = pivotDF.toDF(*new_names).fillna(0)    \n        combined.append(newdf)\n\n    item_feature = join_all(combined,keys)\n    \n    return item_feature","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:27:38.964203Z","iopub.execute_input":"2023-02-22T05:27:38.964782Z","iopub.status.idle":"2023-02-22T05:27:38.976730Z","shell.execute_reply.started":"2023-02-22T05:27:38.964715Z","shell.execute_reply":"2023-02-22T05:27:38.975985Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"items = to_lower(items)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:27:38.978614Z","iopub.execute_input":"2023-02-22T05:27:38.978909Z","iopub.status.idle":"2023-02-22T05:27:39.234520Z","shell.execute_reply.started":"2023-02-22T05:27:38.978878Z","shell.execute_reply":"2023-02-22T05:27:39.233318Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"item_feature = ohe(items)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:27:39.237996Z","iopub.execute_input":"2023-02-22T05:27:39.238437Z","iopub.status.idle":"2023-02-22T05:27:52.421165Z","shell.execute_reply.started":"2023-02-22T05:27:39.238390Z","shell.execute_reply":"2023-02-22T05:27:52.420205Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"transactions = transactions.join(item_feature, on='article_id', how='left').sort('t_dat').drop(*features[1:])","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:27:52.422423Z","iopub.execute_input":"2023-02-22T05:27:52.422711Z","iopub.status.idle":"2023-02-22T05:27:52.796414Z","shell.execute_reply.started":"2023-02-22T05:27:52.422673Z","shell.execute_reply":"2023-02-22T05:27:52.795452Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"dummy_features = transactions.columns[3:]","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:27:52.797649Z","iopub.execute_input":"2023-02-22T05:27:52.797969Z","iopub.status.idle":"2023-02-22T05:27:52.861673Z","shell.execute_reply.started":"2023-02-22T05:27:52.797929Z","shell.execute_reply":"2023-02-22T05:27:52.860880Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"user_feature = transactions.groupBy('customer_id').sum(*dummy_features)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:27:52.863971Z","iopub.execute_input":"2023-02-22T05:27:52.864314Z","iopub.status.idle":"2023-02-22T05:27:53.290869Z","shell.execute_reply.started":"2023-02-22T05:27:52.864270Z","shell.execute_reply":"2023-02-22T05:27:53.289835Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\n\n# def get_pca(df, col):\n    \n    \n#     assembler = VectorAssembler(inputCols=df.columns[1:], outputCol=\"sparse_features\")\n    \n#     feature_vectors = assembler.transform(df).select(*(col, \"sparse_features\"))\n\n\n#     scaler = StandardScaler(inputCol=\"sparse_features\", outputCol=\"scaled_features\")\n#     scalerModel = scaler.fit(feature_vectors)\n\n#     scaled_feature_vectors = scalerModel.transform(feature_vectors).select(*(col, \"scaled_features\"))\n\n#     pca = PCA(k=100, inputCol=\"scaled_features\", outputCol=\"pca\")\n#     pcaModel = pca.fit(scaled_feature_vectors)\n#     x = pcaModel.transform(scaled_feature_vectors).select(*(col, \"pca\"))\n    \n#     return x\n\n\n# user_feature_pca = get_pca(weighted_user_feature, 'customer_id')\n# item_feature_pca = get_pca(item_feature, 'article_id')","metadata":{"execution":{"iopub.status.busy":"2022-05-02T15:25:33.234624Z","iopub.execute_input":"2022-05-02T15:25:33.235273Z","iopub.status.idle":"2022-05-02T15:25:33.240053Z","shell.execute_reply.started":"2022-05-02T15:25:33.235227Z","shell.execute_reply":"2022-05-02T15:25:33.239342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\nfrom pyspark.ml import Pipeline\n\ndef scale(df, col):\n    \n    assembler = VectorAssembler(inputCols=df.columns[1:], outputCol=\"sparse_features\")\n    feature_vectors = assembler.transform(df).select(*(col, \"sparse_features\"))\n\n    scaler = StandardScaler(inputCol=\"sparse_features\", outputCol=\"scaled_features\")\n    scalerModel = scaler.fit(feature_vectors)\n    \n    scaled_feature_vectors = scalerModel.transform(feature_vectors).select(*(col, \"scaled_features\"))\n    \n    return scaled_feature_vectors\n\n\ndef get_pca(df, col):\n    \n    pca = PCA(k=100, inputCol=\"scaled_features\", outputCol=\"pca\")\n    pcaModel = pca.fit(df)\n    \n    return pcaModel","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:27:53.292158Z","iopub.execute_input":"2023-02-22T05:27:53.293604Z","iopub.status.idle":"2023-02-22T05:27:53.303302Z","shell.execute_reply.started":"2023-02-22T05:27:53.293545Z","shell.execute_reply":"2023-02-22T05:27:53.302338Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"scaled_user_feature = scale(user_feature, 'customer_id')\nscaled_item_feature = scale(item_feature, 'article_id')","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:27:53.304847Z","iopub.execute_input":"2023-02-22T05:27:53.305214Z","iopub.status.idle":"2023-02-22T05:29:48.299449Z","shell.execute_reply.started":"2023-02-22T05:27:53.305167Z","shell.execute_reply":"2023-02-22T05:29:48.298784Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"23/02/22 05:27:56 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n","output_type":"stream"},{"name":"stderr","text":"[Stage 115:>                                                        (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:28:12 WARN DAGScheduler: Broadcasting large task binary with size 1091.4 KiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:28:41 WARN DAGScheduler: Broadcasting large task binary with size 1092.0 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 161:============================>                            (2 + 2) / 4]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:28:56 WARN DAGScheduler: Broadcasting large task binary with size 1242.3 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 185:>                                                        (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:29:00 WARN DAGScheduler: Broadcasting large task binary with size 1790.8 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 210:>                                                        (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:29:03 WARN DAGScheduler: Broadcasting large task binary with size 1904.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 262:>                (0 + 1) / 1][Stage 266:>                (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:29:16 WARN DAGScheduler: Broadcasting large task binary with size 1160.9 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 287:==========================================>              (3 + 1) / 4]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:29:47 WARN DAGScheduler: Broadcasting large task binary with size 1196.6 KiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"pca_model = get_pca(scaled_user_feature, 'customer_id')","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:29:48.300369Z","iopub.execute_input":"2023-02-22T05:29:48.300576Z","iopub.status.idle":"2023-02-22T05:30:52.115226Z","shell.execute_reply.started":"2023-02-22T05:29:48.300550Z","shell.execute_reply":"2023-02-22T05:30:52.114445Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"[Stage 334:============================>                            (1 + 1) / 2]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:29:59 WARN DAGScheduler: Broadcasting large task binary with size 1088.3 KiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:30:25 WARN DAGScheduler: Broadcasting large task binary with size 1089.0 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 388:>                                                        (0 + 4) / 4]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:30:40 WARN DAGScheduler: Broadcasting large task binary with size 1239.3 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 412:>                                                        (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:30:43 WARN DAGScheduler: Broadcasting large task binary with size 1668.2 KiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:30:44 WARN DAGScheduler: Broadcasting large task binary with size 1668.2 KiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:30:45 WARN DAGScheduler: Broadcasting large task binary with size 1670.2 KiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:30:46 WARN DAGScheduler: Broadcasting large task binary with size 1668.7 KiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:30:48 WARN DAGScheduler: Broadcasting large task binary with size 1669.4 KiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:30:50 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n23/02/22 05:30:50 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n","output_type":"stream"}]},{"cell_type":"code","source":"user_feature_pca = pca_model.transform(scaled_user_feature)\nitem_feature_pca = pca_model.transform(scaled_item_feature)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:30:52.117050Z","iopub.execute_input":"2023-02-22T05:30:52.117977Z","iopub.status.idle":"2023-02-22T05:30:52.408903Z","shell.execute_reply.started":"2023-02-22T05:30:52.117925Z","shell.execute_reply":"2023-02-22T05:30:52.407996Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"user_feature_pca","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:30:52.409842Z","iopub.execute_input":"2023-02-22T05:30:52.410062Z","iopub.status.idle":"2023-02-22T05:30:52.423185Z","shell.execute_reply.started":"2023-02-22T05:30:52.410035Z","shell.execute_reply":"2023-02-22T05:30:52.422030Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"DataFrame[customer_id: string, scaled_features: vector, pca: vector]"},"metadata":{}}]},{"cell_type":"code","source":"item_feature_pca","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:30:52.424502Z","iopub.execute_input":"2023-02-22T05:30:52.424731Z","iopub.status.idle":"2023-02-22T05:30:52.434703Z","shell.execute_reply.started":"2023-02-22T05:30:52.424701Z","shell.execute_reply":"2023-02-22T05:30:52.434010Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"DataFrame[article_id: string, scaled_features: vector, pca: vector]"},"metadata":{}}]},{"cell_type":"code","source":"# user_feature_pca.write.parquet(\"./user_feature_pca.parquet\")\n# item_feature_pca.write.parquet(\"./item_feature_pca.parquet\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T13:54:22.283471Z","iopub.execute_input":"2022-05-06T13:54:22.284236Z","iopub.status.idle":"2022-05-06T13:54:22.290157Z","shell.execute_reply.started":"2022-05-06T13:54:22.284187Z","shell.execute_reply":"2022-05-06T13:54:22.289353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.feature import BucketedRandomProjectionLSH\nfrom pyspark.sql.functions import col, udf\nimport pyspark.sql.functions as F\n\ndef get_rcmnds(customer, k=12):\n    brp = BucketedRandomProjectionLSH(inputCol=\"pca\", outputCol=\"hashes\", seed=12345, bucketLength=1.0)\n    model = brp.fit(user_feature_pca)\n    temp = model.approxNearestNeighbors(item_feature_pca, customer.pca, k).select('article_id').collect()\n    return temp","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:30:52.436178Z","iopub.execute_input":"2023-02-22T05:30:52.436440Z","iopub.status.idle":"2023-02-22T05:30:52.446290Z","shell.execute_reply.started":"2023-02-22T05:30:52.436410Z","shell.execute_reply":"2023-02-22T05:30:52.444816Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"flagged = rcmnds.join(user_feature_pca.withColumn('flag', F.lit(True)), 'customer_id', 'left').fillna(False)\n\ncold_start = flagged.where('!flag').drop('flag')\nwith_history = flagged.where('flag').drop('flag')","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:30:52.447733Z","iopub.execute_input":"2023-02-22T05:30:52.448157Z","iopub.status.idle":"2023-02-22T05:30:53.221677Z","shell.execute_reply.started":"2023-02-22T05:30:52.448124Z","shell.execute_reply":"2023-02-22T05:30:53.220729Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"rows = with_history.collect()","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:30:53.222882Z","iopub.execute_input":"2023-02-22T05:30:53.223168Z","iopub.status.idle":"2023-02-22T05:32:03.186785Z","shell.execute_reply.started":"2023-02-22T05:30:53.223131Z","shell.execute_reply":"2023-02-22T05:32:03.185800Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"[Stage 571:>                                                        (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:31:07 WARN DAGScheduler: Broadcasting large task binary with size 1091.4 KiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:31:41 WARN DAGScheduler: Broadcasting large task binary with size 1092.0 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 617:============================>                            (2 + 2) / 4]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:31:55 WARN DAGScheduler: Broadcasting large task binary with size 1242.4 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 641:>                                                        (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:31:59 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"rows[0]","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:32:03.188510Z","iopub.execute_input":"2023-02-22T05:32:03.188787Z","iopub.status.idle":"2023-02-22T05:32:03.202154Z","shell.execute_reply.started":"2023-02-22T05:32:03.188727Z","shell.execute_reply":"2023-02-22T05:32:03.200999Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"Row(customer_id='000058a12d5b43e67d225668fa1f8d618c13dc232df0cad8ffe7ad4a1091e318', scaled_features=SparseVector(467, {16: 2.3241, 44: 0.9758, 50: 0.6153, 73: 2.609, 100: 0.4532, 101: 1.0403, 108: 0.6199, 119: 1.8555, 169: 3.4127, 192: 1.578, 382: 1.9109, 387: 0.8273, 436: 2.5315, 464: 2.1834}), pca=DenseVector([-1.5512, 0.7372, 1.9728, -2.3521, -2.3177, 0.2438, 1.5743, -1.258, -2.4537, -0.3919, 1.0904, -0.2851, -0.2111, 0.0758, 0.4033, 0.6975, 0.0137, 0.1488, 0.0697, -0.3862, -0.081, -0.149, -0.1779, -0.0541, -0.0647, 0.1924, 0.0005, 0.8418, 0.5, 0.0659, -0.0915, 0.1752, 0.2201, -0.2583, -0.3694, -0.4289, 0.2696, 0.2568, -0.212, -0.1437, 0.0077, 0.2443, 0.0297, 0.0317, 0.1871, -0.3317, -0.3604, -0.3497, -0.5827, 0.0597, 0.1464, 0.0459, -0.4952, 0.1354, 0.2018, 0.1793, 0.0304, -0.2281, -0.0038, 0.4217, -0.2414, 0.0497, -0.3532, -0.6692, -0.1722, -0.0783, 0.326, -0.689, -0.1874, -0.3543, 0.1921, -0.3941, 0.1569, 0.114, -0.5421, 0.0324, -0.6049, -0.9496, 0.1455, -0.5577, 0.406, -0.0216, 0.5128, -0.305, -0.0712, -0.3063, -0.4399, 0.1177, -0.1522, -0.0867, -0.3426, 0.0825, -0.257, 0.7308, -0.1852, -0.2055, -0.2351, -0.092, 0.173, 0.4255]))"},"metadata":{}}]},{"cell_type":"code","source":"customers = []\nitems = []\nfor row in rows:\n    temp = get_rcmnds(row)\n    customers.append(row[0])\n    items.append(' '.join([i[0] for i in temp]))","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:32:11.992719Z","iopub.execute_input":"2023-02-22T05:32:11.993015Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"[Stage 689:=> (1 + 1) / 2][Stage 695:=> (1 + 1) / 2][Stage 697:>  (0 + 1) / 1]4]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:32:24 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 740:========>        (1 + 1) / 2][Stage 748:>                (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:33:07 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 795:>  (0 + 1) / 1][Stage 797:=> (1 + 1) / 2][Stage 799:>  (0 + 1) / 1]  \r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:33:45 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 842:=> (1 + 1) / 2][Stage 848:=> (1 + 1) / 2][Stage 850:>  (0 + 1) / 1]  \r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:34:19 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 901:>                                                        (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:34:46 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 944:========>        (1 + 1) / 2][Stage 952:>                (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:35:10 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 1003:>                                                       (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:35:35 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 1046:========>       (1 + 1) / 2][Stage 1054:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:36:00 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 1103:========>       (1 + 1) / 2][Stage 1105:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:36:24 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 1156:>                                                       (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:36:48 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 1207:>                                                       (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:37:13 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 1250:========>       (1 + 1) / 2][Stage 1258:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:37:37 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 1309:>                                                       (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:38:01 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 1352:========>       (1 + 1) / 2][Stage 1360:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:38:26 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 1403:========>       (1 + 1) / 2][Stage 1411:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:38:50 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 1462:>                                                       (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:39:14 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 1505:=>(1 + 1) / 2][Stage 1511:=>(1 + 1) / 2][Stage 1513:> (0 + 1) / 1]  \r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:39:38 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 1564:>                                                       (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:40:03 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 1615:>                                                       (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:40:27 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 1666:>                                                       (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:40:53 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 1717:>                                                       (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:41:18 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 1768:>                                                       (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:41:42 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 1819:>                                                       (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:42:07 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 1862:=>(1 + 1) / 2][Stage 1868:=>(1 + 1) / 2][Stage 1870:> (0 + 1) / 1]4]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:42:31 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 1921:>                                                       (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:42:55 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 1970:========>       (1 + 1) / 2][Stage 1972:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:43:19 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 2015:========>       (1 + 1) / 2][Stage 2023:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:43:44 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 2066:========>       (1 + 1) / 2][Stage 2074:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:44:08 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 2117:========>       (1 + 1) / 2][Stage 2125:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:44:33 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 2176:>                                                       (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:44:58 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 2219:=>(1 + 1) / 2][Stage 2225:=>(1 + 1) / 2][Stage 2227:> (0 + 1) / 1]4]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:45:23 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 2270:============================>                           (1 + 1) / 2]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:45:48 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 2321:========>       (1 + 1) / 2][Stage 2329:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:46:12 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 2380:>                                                       (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:46:36 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 2431:>                                                       (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:46:59 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 2474:========>       (1 + 1) / 2][Stage 2482:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:47:24 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 2531:>               (0 + 2) / 2][Stage 2533:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:47:48 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 2584:>                                                       (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:48:13 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 2627:========>       (1 + 1) / 2][Stage 2635:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:48:38 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 2686:>                                                       (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:49:02 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 2735:>               (0 + 2) / 2][Stage 2737:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:49:27 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 2788:>                                                       (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:49:52 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 2837:========>       (1 + 1) / 2][Stage 2839:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:50:16 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 2882:=>(1 + 1) / 2][Stage 2888:> (0 + 2) / 2][Stage 2890:> (0 + 1) / 1]4]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:50:41 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 2941:>                                                       (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:51:06 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 2990:========>       (1 + 1) / 2][Stage 2992:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:51:31 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 3035:============================>                           (1 + 1) / 2]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:51:56 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 3086:=>(1 + 1) / 2][Stage 3092:=>(1 + 1) / 2][Stage 3094:> (0 + 1) / 1]  \r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:52:21 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 3137:========>       (1 + 1) / 2][Stage 3145:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:52:46 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 3188:========>       (1 + 1) / 2][Stage 3196:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:53:12 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 3247:>                                                       (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:53:37 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 3290:========>       (1 + 1) / 2][Stage 3298:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:54:02 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 3341:========>       (1 + 1) / 2][Stage 3349:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:54:28 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 3400:>                                                       (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:54:52 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 3451:>                                                       (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:55:17 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 3494:========>       (1 + 1) / 2][Stage 3502:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:55:41 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 3545:========>       (1 + 1) / 2][Stage 3553:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:56:05 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 3596:========>       (1 + 1) / 2][Stage 3604:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:56:30 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 3655:>                                                       (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:56:54 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 3698:========>       (1 + 1) / 2][Stage 3706:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:57:19 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 3749:========>       (1 + 1) / 2][Stage 3757:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:57:44 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 3806:========>       (1 + 1) / 2][Stage 3808:>               (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:58:10 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 3855:> (0 + 1) / 1][Stage 3857:=>(1 + 1) / 2][Stage 3859:> (0 + 1) / 1]  \r","output_type":"stream"},{"name":"stdout","text":"23/02/22 05:58:34 WARN DAGScheduler: Broadcasting large task binary with size 1489.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nwith_history_df = pd.DataFrame({'customer_id':customers, 'items':items})\nmost_freq = transactions.groupBy('article_id').count().sort(col('count').desc()).limit(12).collect()\n\ndefault = [i[0] for i in most_freq]\ncold_start = cold_start.withColumn('items', lit(' '.join(default))).select(*('customer_id', 'items')).toPandas()\n\ncold_start.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_rcmnds = cold_start.append(with_history_df)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T14:05:34.887681Z","iopub.execute_input":"2022-05-06T14:05:34.888006Z","iopub.status.idle":"2022-05-06T14:05:39.266128Z","shell.execute_reply.started":"2022-05-06T14:05:34.887962Z","shell.execute_reply":"2022-05-06T14:05:39.265114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.evaluation import RegressionEvaluator\n\nevaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"count\",predictionCol=\"prediction\")\nrmse=evaluator.evaluate(all_rcmnds)\nprint(rmse)","metadata":{},"execution_count":null,"outputs":[]}]}